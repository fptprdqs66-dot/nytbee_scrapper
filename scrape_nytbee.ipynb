{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYTBee Scraper\n",
    "\n",
    "This notebook fetches a NYTBee page and extracts the list items from the main answer list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import argparse\n",
    "from html.parser import HTMLParser\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "DEFAULT_URL = \"https://nytbee.com/Bee_20260130.html\"\n",
    "USER_AGENT = \"Mozilla/5.0 (compatible; NYTBeeScraper/1.0)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainAnswerListParser(HTMLParser):\n",
    "    \"\"\"Extract list items from the main answer list.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(convert_charrefs=True)\n",
    "        self._div_depth = 0\n",
    "        self._target_div_depth: Optional[int] = None\n",
    "        self._ul_depth = 0\n",
    "        self._li_stack: list[list[str]] = []\n",
    "        self._items: list[str] = []\n",
    "        self._skip_depth = 0\n",
    "\n",
    "    @property\n",
    "    def items(self) -> list[str]:\n",
    "        return self._items\n",
    "\n",
    "    def handle_starttag(self, tag: str, attrs: list[tuple[str, Optional[str]]]) -> None:\n",
    "        if tag in {\"script\", \"style\"}:\n",
    "            self._skip_depth += 1\n",
    "            return\n",
    "        if tag == \"div\":\n",
    "            self._div_depth += 1\n",
    "            if self._target_div_depth is None and dict(attrs).get(\"id\") == \"main-answer-list\":\n",
    "                self._target_div_depth = self._div_depth\n",
    "            return\n",
    "        if self._target_div_depth is None or self._div_depth < self._target_div_depth:\n",
    "            return\n",
    "        if tag == \"ul\":\n",
    "            self._ul_depth += 1\n",
    "            return\n",
    "        if tag == \"li\" and self._ul_depth:\n",
    "            self._li_stack.append([])\n",
    "\n",
    "    def handle_endtag(self, tag: str) -> None:\n",
    "        if tag in {\"script\", \"style\"} and self._skip_depth:\n",
    "            self._skip_depth -= 1\n",
    "            return\n",
    "        if tag == \"li\" and self._li_stack:\n",
    "            text = \"\".join(self._li_stack.pop()).strip()\n",
    "            if text:\n",
    "                self._items.append(text)\n",
    "            return\n",
    "        if tag == \"ul\" and self._ul_depth:\n",
    "            self._ul_depth -= 1\n",
    "            return\n",
    "        if tag == \"div\":\n",
    "            if self._target_div_depth is not None and self._div_depth == self._target_div_depth:\n",
    "                self._target_div_depth = None\n",
    "            if self._div_depth:\n",
    "                self._div_depth -= 1\n",
    "\n",
    "    def handle_data(self, data: str) -> None:\n",
    "        if self._skip_depth or not self._li_stack:\n",
    "            return\n",
    "        self._li_stack[-1].append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(url: str, timeout: int = 20) -> str:\n",
    "    request = Request(url, headers={\"User-Agent\": USER_AGENT})\n",
    "    with urlopen(request, timeout=timeout) as response:\n",
    "        charset = response.headers.get_content_charset() or \"utf-8\"\n",
    "        return response.read().decode(charset, errors=\"replace\")\n",
    "\n",
    "\n",
    "def extract_answer_list(html: str) -> list[str]:\n",
    "    parser = MainAnswerListParser()\n",
    "    parser.feed(html)\n",
    "    return parser.items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and extract answers\n",
    "\n",
    "Set `url` to the NYTBee page you want to scrape, then run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://nytbee.com/Bee_{date}.html\"\n",
    "log_path = \"nytbee_scrape.log\"\n",
    "dict_path = \"nytbee_dict.txt\"\n",
    "\n",
    "start_input = input(\"Enter start date (YYYY-MM-DD) [default: today]: \").strip()\n",
    "if start_input:\n",
    "    starting_date = date.fromisoformat(start_input)\n",
    "else:\n",
    "    starting_date = date.today()\n",
    "\n",
    "while True:\n",
    "    days_input = input(\"Enter number of days to collect: \").strip()\n",
    "    try:\n",
    "        days_to_collect = int(days_input)\n",
    "        if days_to_collect <= 0:\n",
    "            raise ValueError(\"Number of days must be positive\")\n",
    "    except ValueError:\n",
    "        print(\"Please enter a positive integer for the number of days.\")\n",
    "        continue\n",
    "    break\n",
    "\n",
    "word_counts: dict[str, int] = {}\n",
    "failed_urls: list[tuple[str, object]] = []\n",
    "\n",
    "try:\n",
    "    with open(dict_path, \"r\", encoding=\"utf-8\") as file_handle:\n",
    "        contents = file_handle.read().strip()\n",
    "    if contents:\n",
    "        from ast import literal_eval\n",
    "\n",
    "        loaded = literal_eval(contents)\n",
    "        if isinstance(loaded, dict):\n",
    "            word_counts = {str(key): int(value) for key, value in loaded.items()}\n",
    "            print(f\"Loaded {len(word_counts)} words from {dict_path}.\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "except (ValueError, SyntaxError) as exc:\n",
    "    print(f\"Warning: Could not parse {dict_path}: {exc}\")\n",
    "\n",
    "scraped_urls: set[str] = set()\n",
    "try:\n",
    "    with open(log_path, \"r\", encoding=\"utf-8\") as file_handle:\n",
    "        scraped_urls = {line.strip() for line in file_handle if line.strip()}\n",
    "    if scraped_urls:\n",
    "        print(f\"Loaded {len(scraped_urls)} scraped URLs from {log_path}.\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with open(log_path, \"a\", encoding=\"utf-8\") as log_handle:\n",
    "    with tqdm(range(days_to_collect), desc=\"Collecting\", unit=\"day\") as progress:\n",
    "        for offset in progress:\n",
    "            target_date = starting_date - timedelta(days=offset)\n",
    "            url = base_url.format(date=target_date.strftime(\"%Y%m%d\"))\n",
    "\n",
    "            if url in scraped_urls:\n",
    "                progress.set_postfix({\"status\": \"skipped\", \"url\": url})\n",
    "                continue\n",
    "\n",
    "            progress.set_postfix({\"status\": \"collecting\", \"url\": url})\n",
    "            try:\n",
    "                html = fetch_html(url, timeout=20)\n",
    "            except (HTTPError, URLError) as exc:\n",
    "                failed_urls.append((url, exc))\n",
    "                progress.set_postfix({\"status\": f\"failed: {exc}\", \"url\": url})\n",
    "                continue\n",
    "\n",
    "            items = extract_answer_list(html)\n",
    "            if not items:\n",
    "                failed_urls.append((url, \"No answers extracted\"))\n",
    "                progress.set_postfix({\"status\": \"failed: no answers\", \"url\": url})\n",
    "                continue\n",
    "\n",
    "            for item in items:\n",
    "                word = item.strip()\n",
    "                if word:\n",
    "                    word_counts[word] = word_counts.get(word, 0) + 1\n",
    "            scraped_urls.add(url)\n",
    "            log_handle.write(f\"{url}\")\n",
    "",
    "            log_handle.flush()\n",
    "            progress.set_postfix({\"status\": f\"collected {len(items)}\", \"url\": url})\n",
    "\n",
    "print(\n",
    "    f\"Collected {len(word_counts)} distinct words from {days_to_collect - len(failed_urls)} days.\"\n",
    ")\n",
    "\n",
    "with open(dict_path, \"w\", encoding=\"utf-8\") as file_handle:\n",
    "    file_handle.write(str(word_counts))\n",
    "\n",
    "print(\"\n",
    "First 100 words in the dictionary:\")\n",
    "for word in sorted(word_counts)[:100]:\n",
    "    print(word)\n",
    "\n",
    "if failed_urls:\n",
    "    print(\"\n",
    "Skipped the following URLs:\")\n",
    "    for url, reason in failed_urls:\n",
    "        print(f\"- {url} ({reason})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
