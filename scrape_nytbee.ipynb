{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYTBee Scraper\n",
    "\n",
    "This notebook fetches a NYTBee page and extracts the list items from the main answer list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/fptprdqs66-dot/nytbee_scrapper.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "from tqdm import tqdm\n",
    "from urllib.error import HTTPError, URLError\n",
    "\n",
    "from nytbee_scrapper import BASE_URL, extract_answer_list, fetch_html, normalize_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and extract answers\n",
    "\n",
    "Set `url` to the NYTBee page you want to scrape, then run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = BASE_URL\n",
    "log_path = \"nytbee_scrape.log\"\n",
    "dict_path = \"nytbee_dict.txt\"\n",
    "\n",
    "start_input = input(\"Enter start date (YYYY-MM-DD) [default: today]: \").strip()\n",
    "if start_input:\n",
    "    starting_date = date.fromisoformat(start_input)\n",
    "else:\n",
    "    starting_date = date.today()\n",
    "\n",
    "while True:\n",
    "    days_input = input(\"Enter number of days to collect: \").strip()\n",
    "    try:\n",
    "        days_to_collect = int(days_input)\n",
    "        if days_to_collect < 0:\n",
    "            raise ValueError(\"Number of days must be positive\")\n",
    "    except ValueError:\n",
    "        print(\"Please enter a non-negative integer for the number of days.\")\n",
    "        continue\n",
    "    break\n",
    "\n",
    "word_counts: dict[str, int] = {}\n",
    "failed_urls: list[tuple[str, object]] = []\n",
    "\n",
    "try:\n",
    "    with open(dict_path, \"r\", encoding=\"utf-8\") as file_handle:\n",
    "        contents = file_handle.read().strip()\n",
    "    if contents:\n",
    "        if contents.lstrip().startswith(\"{\"):\n",
    "            from ast import literal_eval\n",
    "\n",
    "            loaded = literal_eval(contents)\n",
    "            if isinstance(loaded, dict):\n",
    "                word_counts = {str(key): int(value) for key, value in loaded.items()}\n",
    "                print(f\"Loaded {len(word_counts)} words from {dict_path}.\")\n",
    "        else:\n",
    "            words = [line.strip().lower() for line in contents.splitlines()]\n",
    "            word_counts = {word: 1 for word in words if word}\n",
    "            if word_counts:\n",
    "                print(f\"Loaded {len(word_counts)} words from {dict_path}.\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "except (ValueError, SyntaxError) as exc:\n",
    "    print(f\"Warning: Could not parse {dict_path}: {exc}\")\n",
    "\n",
    "scraped_urls: set[str] = set()\n",
    "try:\n",
    "    with open(log_path, \"r\", encoding=\"utf-8\") as file_handle:\n",
    "        scraped_urls = {line.strip() for line in file_handle if line.strip()}\n",
    "    if scraped_urls:\n",
    "        print(f\"Loaded {len(scraped_urls)} scraped URLs from {log_path}.\")\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with open(log_path, \"a\", encoding=\"utf-8\") as log_handle:\n",
    "    with tqdm(range(days_to_collect), desc=\"Collecting\", unit=\"day\") as progress:\n",
    "        for offset in progress:\n",
    "            target_date = starting_date - timedelta(days=offset)\n",
    "            url = base_url.format(date=target_date.strftime(\"%Y%m%d\"))\n",
    "\n",
    "            if url in scraped_urls:\n",
    "                progress.set_postfix({\"status\": \"skipped\", \"url\": url})\n",
    "                continue\n",
    "\n",
    "            progress.set_postfix({\"status\": \"collecting\", \"url\": url})\n",
    "            try:\n",
    "                html = fetch_html(url, timeout=20)\n",
    "            except (HTTPError, URLError) as exc:\n",
    "                failed_urls.append((url, exc))\n",
    "                progress.set_postfix({\"status\": f\"failed: {exc}\", \"url\": url})\n",
    "                continue\n",
    "\n",
    "            items = extract_answer_list(html)\n",
    "            if not items:\n",
    "                failed_urls.append((url, \"No answers extracted\"))\n",
    "                progress.set_postfix({\"status\": \"failed: no answers\", \"url\": url})\n",
    "                continue\n",
    "\n",
    "            for item in items:\n",
    "                word = normalize_answer(item)\n",
    "                if word:\n",
    "                    word_counts[word] = word_counts.get(word, 0) + 1\n",
    "            scraped_urls.add(url)\n",
    "            log_handle.write(f\"{url}\\n\")\n",
    "            log_handle.flush()\n",
    "            progress.set_postfix({\"status\": f\"collected {len(items)}\", \"url\": url})\n",
    "\n",
    "print(f\"Collected {len(word_counts)} distinct words from {days_to_collect - len(failed_urls)} days.\")\n",
    "\n",
    "with open(dict_path, \"w\", encoding=\"utf-8\") as file_handle:\n",
    "    for word in sorted(word_counts):\n",
    "        file_handle.write(f\"{word}\\n\")\n",
    "\n",
    "print(\"First 100 words in the dictionary:\")\n",
    "for word in sorted(word_counts)[:100]:\n",
    "    print(word)\n",
    "\n",
    "if failed_urls:\n",
    "    print(\"\\nSkipped the following URLs:\")\n",
    "    for url, reason in failed_urls:\n",
    "        print(f\"- {url} ({reason})\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
